---
title: "Support Vector Machine: Linear SVM"
subtitle: "The Office Rating Prediction"
author: "Jesús Calderón"
date: "20/03/2021"
output: 
  html_document:
    theme: flatly
    toc: True
    toc_float: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE)
```

# Main Points

In this document we will discuss:

+ How to tune a Linear Support Vector Machine Model.
+ How to refine a grid search under a two-parameter search. 

# Load Libraries and Data

We load the data that we prepared previously. You may need to `install.packages('baguette')`.

```{r}
library(tidyverse)
library(tidymodels)
library(doParallel)
library(knitr)
library(scales)
library(tictoc)

load('./data/office.Rda')
```

## Metrics

Prepare a metric set of RMSE, MAE, and R-Squared. We will use RMSE as a decision metric.

```{r}
perf_metrics <- metric_set(rmse, mae, rsq) 
```


## Create Train, Test Data, Resamples

Create initial and CV splits. 

```{r}
set.seed(123)
office_split <- initial_split(office_dt, prop = 0.7, strata = season)
office_train <- training(office_split)
office_test <- testing(office_split)
```

```{r}
set.seed(456)
office_folds <- office_train %>%
  vfold_cv(v = 5, repeats = 5, strata = season)
```



# Build a Support Vector Machine Workflow

For preprocessing steps, you can start with the recommendations in the [Appendix to Khun and Silge (2021)](https://www.tmwr.org/pre-proc-table.html).

## Recipe

```{r}
office_svm_rec <- 
  recipe(imdb_rating ~ ., data = office_train) %>%
  update_role(episode_name, new_role = 'ID') %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_normalize(andy:jan, -all_outcomes()) %>%
  step_corr(andy:jan, -all_outcomes()) %>%
  step_BoxCox(andy:jan, -all_outcomes())
```

# Model: Linear SVM

The model that we will work with is `svm_linear()`, a linear SVM. 

The function documentation explicitly states that "the LiblineaR engine cannot produce class probabilities", therefore we will need to choose another engine to work with class probabilities (e.g., kernlab). 

The tuning parameters are:

  - `cost`: cost of predicting a sample within or on the wrong side of the margin.
  - `margin`: the epsilon in the SVM insensitive loss function (regression only).

You may need to upgrade the parsnip library to version 0.1.6 or higher to run the following code.

```{r}
svm_lin_mod <- 
  svm_linear(cost = tune(), 
             margin = tune()) %>%
  set_mode('regression') %>%
  set_engine('kernlab')
```


# Tuning SVM RBF: Grid Search Approach

We start by searching through the regular grid given by the standard values in library dials. We start with five levels of each hyperparameter: cost and margin.

```{r}
svm_lin_grid <- grid_regular(cost(), 
                             svm_margin(),
                             levels = 5)
```


# Tuning Linear SVM using Grid Search

We create the workflow.

```{r}
svm_lin_wf <- 
  workflow() %>%
  add_recipe(office_svm_rec) %>%
  add_model(svm_lin_mod)
```


Tune the model using the grid.

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

We time the process using the library tictoc.

```{r}
tune_ctrl <- control_resamples(save_pred = TRUE)

tic()
svm_lin_res <-
  svm_lin_wf %>%
  tune_grid(
    office_folds, 
    grid = svm_lin_grid,
    metrics = perf_metrics, 
    control = tune_ctrl
  )
toc()
```

```{r, echo = TRUE}
# Remember to add this if you are running parallel computations.
stopCluster(cl)
```

## Initial Results

```{r}
svm_lin_res %>%
  collect_metrics() %>%
  ggplot(aes(x = cost, 
             y = mean,
             colour = margin,
             group = margin)) +
  geom_line() + 
  geom_point() + 
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err)
  ) + 
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine', 
       subtitle = 'Tuning Cost and Margin', 
       x = 'Cost', y = 'Performance Score')
```

## Refinements

We adjust the experiment's parameters and try to optimize further:

```{r}
svm_lin_grid_2 <- grid_regular(
  cost(range = c(-10, 0)), 
  svm_margin(range = c(0.1, 0.3)),
  levels = c(10,5)
)
```

Tune the model using the new grid.

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)

tic()
svm_lin_res_2 <-
  svm_lin_wf %>%
  tune_grid(
    office_folds, 
    grid = svm_lin_grid_2,
    metrics = perf_metrics, 
    control = tune_ctrl
  )
toc()

stopCluster(cl)
```

Produce refined results.

```{r}
svm_lin_res_2 %>%
  collect_metrics() %>%
  ggplot(aes(x = cost, 
             y = mean,
             colour = margin,
             group = margin)) +
  geom_line() + 
  geom_point() + 
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err)
  ) + 
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine', 
       subtitle = 'Tuning Cost and Margin', 
       x = 'Cost', y = 'Performance Score')
```

A different visualization.

```{r}
svm_lin_res_2 %>%
  collect_metrics() %>%
  filter(.metric != 'rsq') %>%
  ggplot(aes(x = margin, 
             y = cost,
             z = mean)) +
  geom_contour_filled() + 
  scale_y_log10() + 
  facet_wrap(~.metric, ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine', 
       subtitle = 'Tuning Cost and Margin', 
       x = 'Margin', y = 'Cost')
```

The top performing models by RMSE are:

```{r}
svm_lin_res %>%
  show_best(metric = 'rmse') %>%
  kable(digits = 3,
        caption = 'Top-Performing Linear SVM Models by RMSE')
```


# Model Selection

We select the best model by RMSE.

```{r}
svm_lin_best <- 
  svm_lin_res_2 %>%
  select_best( metric = 'rmse')
svm_lin_best %>%
  kable(digits = 3, 
        caption = 'Top-Performing Model by RMSE')
```


```{r}
svm_lin_by_se <- 
  svm_lin_res_2 %>%
  select_by_one_std_err(metric = 'rmse', 'penalty')
svm_lin_by_se %>%
  kable(digits = 3, 
        caption = 'Top Performing Model Adjusted by SE (RMSE)')
```

```{r}
svm_lin_res_2 %>%
  collect_predictions(parameters = svm_lin_best) %>%
  ggplot(aes(x = imdb_rating, y = .pred)) + 
  geom_point(alpha=.25, 
             color = 'skyblue4') + 
  geom_smooth(method = 'loess', 
              se = FALSE,
              colour = 'orangered3') + 
  geom_abline(slope = 1, 
              intercept = 0, linetype = 2) +
  scale_x_continuous(breaks = breaks_extended(n = 15)) + 
  scale_y_continuous(breaks = breaks_extended(n = 7)) + 
  geom_rug(alpha = 0.25) + 
  theme_minimal() +
  labs(title = 'Predicted and Actual IMBD Ratings', 
       subtitle = 'Loess Smoothing (solid) and 45 Degree Line (dotted)', 
       x = 'IMBD Rating', y = 'Predicted Rating')
```

# Finalize Workflow and Train

```{r}
svm_lin_final <-
  svm_lin_wf %>%
  finalize_workflow(svm_lin_best)

svm_lin_fit <- svm_lin_final %>%
  fit(office_train)

svm_lin_fit
```

# Save Model Objects

```{r}
save(svm_lin_fit, svm_lin_res, svm_lin_wf, 
     file = './data/svm_lin_res.Rda')
```
