---
title: "Support Vector Machine: RBF SVM"
subtitle: "The Office Rating Prediction"
author: "Jesús Calderón"
date: "20/03/2021"
output: 
  html_document:
    theme: flatly
    toc: True
    toc_float: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE)
```

# Main Points

In this document we will discuss:

+ How to tune an RBF SVM model.
+ How to tune a parameters using Latin Hypercubes to get a rough estimate and then fine-tune with regular grid.

# Load Libraries and Data

We load the data that we prepared previously. You may need to `install.packages('baguette')`.

```{r}
library(tidyverse)
library(tidymodels)
library(doParallel)
library(knitr)
library(scales)
library(tictoc)

load('./data/office.Rda')
```

## Metrics

Prepare a metric set of RMSE, MAE, and R-Squared. We will use RMSE as a decision metric.

```{r}
perf_metrics <- metric_set(rmse, mae, rsq) 
```


## Create Train, Test Data, Resamples

Create initial and CV splits. 

```{r}
set.seed(123)
office_split <- initial_split(office_dt, prop = 0.7, strata = season)
office_train <- training(office_split)
office_test <- testing(office_split)
```

```{r}
set.seed(456)
office_folds <- office_train %>%
  vfold_cv(v = 5, repeats = 5, strata = season)
```



# Build a Support Vector Machine Workflow

For preprocessing steps, you can start with the recommendations in the [Appendix to Khun and Silge (2021)](https://www.tmwr.org/pre-proc-table.html).

## Recipe

```{r}
office_svm_rec <- 
  recipe(imdb_rating ~ ., data = office_train) %>%
  update_role(episode_name, new_role = 'ID') %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_normalize(andy:jan, -all_outcomes()) %>%
  step_corr(andy:jan, -all_outcomes()) %>%
  step_BoxCox(andy:jan, -all_outcomes())
```

# Model: SVM with RBF kernel

The model that we will work with is  `svm_rbf()`, a SVM using a Radial Basis Function (RBF). The tuning parameters are:

  - `cost`: cost of predicting a sample within or on the wrong side of the margin.
  - `margin`: the epsilon in the SVM insensitive loss function (regression only).
  - `rbf_sigma`: the precision parameter for the radial basis function.

You may need to upgrade the parsnip library to version 0.1.6 or higher to run the following code.

```{r}
svm_rbf_mod <- 
  svm_rbf(rbf_sigma = tune(), 
          margin = tune()) %>%
  set_mode('regression') %>%
  set_engine('kernlab')
```


# Tuning SVM RBF: Latin Hypercube

In this case, we can start our search with a Latin Hypercube and continue with a grid search focused on areas that appear to be promising.

```{r}
svm_rbf_grid <- grid_latin_hypercube(
  rbf_sigma(),
  svm_margin(),
  size = 10
)
```


# Tuning Linear SVM using Grid Search

We create the workflow.

```{r}
svm_rbf_wf <- 
  workflow() %>%
  add_recipe(office_svm_rec) %>%
  add_model(svm_rbf_mod)
```


Tune the model using the grid.

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```

We time the process using the library tictoc.

```{r}
tune_ctrl <- control_resamples(save_pred = TRUE)

tic()
svm_rbf_res <-
  svm_rbf_wf %>%
  tune_grid(
    office_folds, 
    grid = svm_rbf_grid,
    metrics = perf_metrics, 
    control = tune_ctrl
  )
toc()
```

```{r, echo = TRUE}
# Remember to add this if you are running parallel computations.
stopCluster(cl)
```

## Initial Results

```{r}
svm_rbf_res %>%
  collect_metrics() %>%
  ggplot(aes(x = rbf_sigma, 
             y = mean,
             colour = margin,
             group = margin)) +
  geom_point() + 
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err)
  ) + 
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine', 
       subtitle = 'Tuning Cost and Margin', 
       x = 'RBF Sigma', y = 'Performance Score')
```

## Refinements

We adjust the experiment's parameters and try to optimize further:

```{r}
svm_rbf_grid_2 <- grid_regular(
  rbf_sigma(range = c(-3, 0)),
  svm_margin(range = c(0.1, 0.3)),
  levels = c(10, 5)
)
```

Tune the model using the new grid.

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)

tic()
svm_rbf_res_2 <-
  svm_rbf_wf %>%
  tune_grid(
    office_folds, 
    grid = svm_rbf_grid_2,
    metrics = perf_metrics, 
    control = tune_ctrl
  )
toc()

stopCluster(cl)
```

Produce refined results.

```{r}
svm_rbf_res_2 %>%
  collect_metrics() %>%
  ggplot(aes(x = rbf_sigma, 
             y = mean,
             colour = margin,
             group = margin)) +
  geom_line() + 
  geom_point() + 
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err)
  ) + 
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine', 
       subtitle = 'Tuning Cost and Margin', 
       x = 'Cost', y = 'Performance Score')
```

A different visualization.

```{r}
svm_rbf_res_2 %>%
  collect_metrics() %>%
  filter(.metric != 'rsq') %>%
  ggplot(aes(x = margin, 
             y = rbf_sigma,
             z = mean)) +
  geom_contour_filled() + 
  scale_y_log10() + 
  facet_wrap(~.metric, ncol = 2) +
  theme_minimal() + 
  labs(title = 'Support Vector Machine', 
       subtitle = 'Tuning Cost and Margin', 
       x = 'Margin', y = 'RBF Sigma')
```

The top performing models by RMSE are:

```{r}
svm_rbf_res_2 %>%
  show_best(metric = 'rmse') %>%
  kable(digits = 3,
        caption = 'Top-Performing Linear SVM Models by RMSE')
```


# Model Selection

We select the best model by RMSE.

```{r}
svm_rbf_best <- 
  svm_rbf_res_2 %>%
  select_best( metric = 'rmse')
svm_rbf_best %>%
  kable(digits = 3, 
        caption = 'Top-Performing Model by RMSE')
```


```{r}
svm_rbf_by_se <- 
  svm_rbf_res_2 %>%
  select_by_one_std_err(metric = 'rmse', 'rbf_sigma')
svm_rbf_by_se %>%
  kable(digits = 3, 
        caption = 'Top Performing Model Adjusted by SE (RMSE)')
```

```{r}
svm_rbf_res_2 %>%
  collect_predictions(parameters = svm_rbf_best) %>%
  ggplot(aes(x = imdb_rating, y = .pred)) + 
  geom_point(alpha=.25, 
             color = 'skyblue4') + 
  geom_smooth(method = 'loess', 
              se = FALSE,
              colour = 'orangered3') + 
  geom_abline(slope = 1, 
              intercept = 0, linetype = 2) +
  scale_x_continuous(breaks = breaks_extended(n = 15)) + 
  scale_y_continuous(breaks = breaks_extended(n = 7)) + 
  geom_rug(alpha = 0.25) + 
  theme_minimal() +
  labs(title = 'Predicted and Actual IMBD Ratings', 
       subtitle = 'Loess Smoothing (solid) and 45 Degree Line (dotted)', 
       x = 'IMBD Rating', y = 'Predicted Rating')
```

# Finalize Workflow and Train

```{r}
svm_rbf_final <-
  svm_rbf_wf %>%
  finalize_workflow(svm_rbf_best)

svm_rbf_fit <- svm_rbf_final %>%
  fit(office_train)

svm_rbf_fit
```

# Save Model Objects

```{r}
save(svm_rbf_fit, svm_rbf_res, svm_rbf_wf, 
     file = './data/svm_rbf_res.Rda')
```
