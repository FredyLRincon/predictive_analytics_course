---
title: "Feature Engineering"
author: "Jesús Calderón"
output: 
  html_document:
    toc: FALSE
    toc_float: TRUE
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# What is Feature Engineering?

The purpose of feature engineering is to *transform and represent features so that their information content is best exposed* to the learning algorithm. Feature engineering can include:

* A transformation of a predictor: standardization, scale, center, log, etc.
* An equivalent re-representation of a predictor: dummy variables, one-hot-encoding, binning, etc.
* An interaction of two or more predictors such as a product or ratio: for example, calculate the ratio of a loan to the value of the collateral (or its inverse), as a new feature for default prediction.
* A functional relationship among predictors: Principal Components Analysis, LDA, etc. This may also include methods for imputing missing values.


# Tranformations

A function can be applied to the values of a predictor  that bring out important features in the data. For example, the log transformation can be used to [magnify small quantities and shrink large quantities](https://xkcd.com/482/). For instance, the following example combines many small values and a few large values. We do the following:

+ Create 2,000 observations with small mean and standard deviation.
+ Create 50 observations with large mean and standard deviation.
+ Combine them using `c()` *and then*...
+ Shuffle the observations. Here, I use the command `sample_n()` and give it a sample size equal to the total size of the tibble.
+ Finally, add a sequential `x`.


```{r }
library(tidyverse)
library(knitr)
sizes_dt <- tibble(x = rlnorm(10000, 0, 1))

sizes_dt %>%
  ggplot(aes(x = x)) + 
  geom_histogram(bins = 100) +
  labs(title = 'Data with different orders of magnitude', 
       subtitle = 'No transformation') +
  theme_minimal()

```


## Recipes

As described in Chapter 6, [Feature engineering with recipes](https://www.tmwr.org/recipes.html), of Khun and Silge (2021), we can use recipes to prepare our data. In this case, we want to transform y by applying a log transformation to get better visibility of small values and shrink larger values. In particular, it is essential to remember that (Kuhn and Silge, 2021):

> A recipe is also an object that defines a series of steps for data processing. Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done. 

The recipe below is simple:

+ Take `x` from the data.
+ Convert it by taking the log base 10.
+ Prep the recipe. This performs the calculations and produces a new data set. (Accordingly, we change assignment, `<-`,  notation.)
+ Show the results.


```{r log_trans}
library(tidymodels)

sizes_recipe <-
  recipe(~ x, data = sizes_dt) %>%
  step_log(x, base=10) 

sizes_rec_prepped <- prep(sizes_recipe)
sizes_rec_prepped <- bake(sizes_rec_prepped, 
                          new_data = NULL)

sizes_rec_prepped %>%
  ggplot(aes(x = x)) +
  geom_histogram(bins = 100) +
  labs(title = 'Transformed data')
```

# Normalization

We can use a normalization step to center and scale our variables:

+ Centering a variable involves subtracting its mean from all of its value. Centred variables have a mean value of zero.
+ Scaling a variable typically involves dividing every value by their standard deviation. Scaled variables have a standard deviation value of one.

When we apply both centring and scaling, we say that we *normalize* or *standardize* it.

In `tidymodels` we can use `step_normalize()`, `step_center()`, and `step_scale()`. For example,

```{r normalize}
rho = 0.65

dt <- tibble(x = rnorm(1000, 100, 50))

dt %>%
  ggplot(aes(x = x)) + 
  geom_histogram(bins = 50) + 
  labs(title = 'Normal Data', 
       subtitle = 'Mean: 100 | SD: 50') +
  theme_minimal()
```

Notice how, after normalization, the center of the distributions shifts from around 100 units to 0. Likewise, the variability of our variable is reduced.

```{r}
norm_recipe <-
  recipe(~ x , data = dt) %>%
  step_normalize(x) 

norm_rec_prep <- prep(norm_recipe) %>%
  bake(new_data = NULL)

norm_rec_prep %>%
  ggplot(aes(x = x)) + 
  geom_histogram(bins = 50) + 
  labs('Normalized Data') + 
  theme_minimal()

```


Normalization changes the scale of the distribution. It makes variables a "standard size". Normalization benefits algorithms that are scale sensitive and generally do not hurt algorithms that are scale insensitive. There is little downside to scaling features, in general.


# Recoding Variables

From the perspective of variables and values, we sometimes talk about *code* or, more frequently, how a variable is *encoded*. We think of the *encoding of a variable* as the representation of the data. Some of the models that we use will often benefit or will require that data be *encoded* in a form that is better suited for the model's inputs. Encoding (or recoding) generally involves changing a variable type: creating dummy variables implies converting information in a categorical variable to a numeric variable, for example.

## Dummy Variables

Dummy variables are a form of encoding categorical data. For example, if we had a data set that includes a variable "day of the week" with 5 values, Monday through Friday, we can re-code it as a dummy as follows:

```{r dummy}
workday <- tibble(day_of_week = c('Mon', 
                                  'Tue',
                                  'Wed',
                                  'Thu',
                                  'Fri'))


dummy_rec <-
  recipe(~ day_of_week , data = workday) %>%
  step_dummy(day_of_week) 

dummy_rec_prep <- prep(dummy_rec) %>%
  bake(new_data = NULL)

dummy_rec_prep %>%
  kable()
```

There are several things to notice from the result above:

+ Dummy variables are binary: they will take a value of 0 or 1.
+ Dummy variables are numerical and not categorical, factor, boolean, etc.
+ If the original variable contained C levels, then we will get C-1 levels by default. For instance, our example had five levels (one per weekday), but the resulting dummy representation only has four. The reason is that we can back out the fifth value since we know that when all four values are 0, then the fifth value should be 1. This avoids an undesirable situation for some methods called colinearity (one variable can be obtained as a linear function of others). Colinearity is one form of observing information redundancy.
+ When recoding into dummy variables, the first value is dropped.
+ If the original value is missing, then all dummy variables are `NA`.
+ If the data contains a novel value (a value that it hand not yet considered and encoded), then all values will be `NA`. For these cases, we can also consider `step_other()`.

## One-Hot Encoding

Some algorithms require one-hot encoding instead of dummy variables. **One-hot encoding** is similar to dummy variable, but all values receive a column. In our example, we would get five columns and not only four.

```{r one_hot}
workday <- tibble(day_of_week = c('Mon', 
                                  'Tue',
                                  'Wed',
                                  'Thu',
                                  'Fri'))


onehot_rec <-
  recipe(~ day_of_week , data = workday) %>%
  step_dummy(day_of_week, one_hot = TRUE) 

onehot_rec_prep <- prep(onehot_rec) %>%
  bake(new_data = NULL)

onehot_rec_prep %>%
  kable()
```

## Binning (Discretization)

One simple way that we can use to transform a numeric variable into a categorical variable is to place each value in a bin:

+ `step_discretize()` will create equal-weighted bins with an approximately equal number of points.
+ `step_cut()` will create bins based on provided boundary levels. 

```{r}
dt <- tibble(x = seq(1:100))

discretize_rec <- 
  recipe(dt) %>%
  step_discretize(x, num_breaks = 7)

discretize_rec_prep <- prep(discretize_rec) %>%
  bake(new_data = NULL)

discretize_rec_prep %>%
  table()
```

```{r}
dt <- tibble(x = seq(1:100))

discretize_rec <- 
  recipe(dt) %>%
  step_cut(x, breaks = c(7, 25, 70, 90))

discretize_rec_prep <- prep(discretize_rec) %>%
  bake(new_data = NULL)

discretize_rec_prep %>%
  table()
```
# Interactions

We may also benefit from establishing interaction terms. This type of transformation is primarily intended for numeric data but may be applied to categorical data after being transformed into dummy variables.

Interaction variables typically are used to capture the joint contribution to our predictive model of two or more variables after accounting for their individual contributions. A majority of cases will result in the model benefiting only marginally from these terms; however, they are fundamental in some contexts: for example, loan value and collateral value are typically included in default prediction models, together with their interaction term. 

You can specify interaction variables using `step_interact()` and denoting the interaction with `:`, for instance `step_interact(~loan:collateral)`. 

# Multivariate Transformations

Some transformations may include more complex formulations or the results of models that we use to pre-process the data. A couple of examples include:

+ `step_pca()` which extracts the principal components of a data set.
+ `step_classdist()` calculates the distance to class centroids. A *centroid* is the element-wise average of a set of data points. 


# Other Useful Functions for Feature Engineering

Other useful functions for feature engineering include:

+ If we had a case in which labels are encoded as numbers, we can use `step_num2factor()` to convert into factor.
+ `step_corr()` filters out highly correlated variables (reduces redundancyy).
+ `step_nzv()` removes variables with low variability.


As well, please refer to the [function reference for all the step functions](https://recipes.tidymodels.org/reference/index.html) available through tidymodels.


