---
title: "Neural Nets"
subtitle: "The Office Rating Prediction"
author: "Jesús Calderón"
date: "20/03/2021"
output: 
  html_document:
    theme: flatly
    toc: True
    toc_float: True
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = TRUE)
```

# Main Points

In this document we will discuss:

+ How to tune a Neural Net Model.

# Load Libraries and Data

We load the data that we prepared previously. 

```{r}
library(tidyverse)
library(tidymodels)
library(doParallel)
library(knitr)
library(scales)
library(tictoc)

load('./data/office.Rda')
```

## Metrics

Prepare a metric set of RMSE, MAE, and R-Squared. We will use RMSE as a decision metric.

```{r}
perf_metrics <- metric_set(rmse, mae, rsq) 
```


## Create Train, Test Data, Resamples

Create initial and CV splits. 

```{r}
set.seed(123)
office_split <- initial_split(office_dt, prop = 0.7, strata = season)
office_train <- training(office_split)
office_test <- testing(office_split)
```

```{r}
set.seed(456)
office_folds <- office_train %>%
  vfold_cv(v = 5, repeats = 5, strata = season)
```



# Build a Neural Network

For preprocessing steps, you can start with the recommendations in the [Appendix to Khun and Silge (2021)](https://www.tmwr.org/pre-proc-table.html).

## Recipe

```{r}
office_nn_rec <- 
  recipe(imdb_rating ~ ., data = office_train) %>%
  update_role(episode_name, new_role = 'ID') %>%
  step_zv(all_numeric(), -all_outcomes()) %>%
  step_normalize(andy:jan, -all_outcomes()) %>%
  step_corr(andy:jan, -all_outcomes()) %>%
  step_BoxCox(andy:jan, -all_outcomes())
```

# Model: Neural Net

The model that we will work with is `mlp()` which produces a single-layer neural net. 

The tuning parameters are:

  - `hidden_units`: the number of hidden units in the layer.
  - `penalty`: the amount of L2 regularization. Also known as 'weight decay'.
  - `epochs`: the number of training iterations.



```{r}
nn_mod <- 
  mlp(hidden_units = tune(),
      penalty = tune(),
      epochs = tune()) %>%
  set_mode('regression') %>%
  set_engine('nnet')
```


# Tuning 

We start by searching through the regular grid given by the standard values in library dials. 

```{r}
nn_grid <- grid_latin_hypercube(hidden_units(range = c(3, 15)), 
                             penalty(),
                             epochs(range = c(10, 250)), 
                             size = 10)

nn_wf <- 
  workflow() %>%
  add_recipe(office_nn_rec) %>%
  add_model(nn_mod)
```


Tune the model using the grid. We time the process using the library tictoc.

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)

tune_ctrl <- control_resamples(save_pred = TRUE)

tic()
nn_res <-
  nn_wf %>%
  tune_grid(
    office_folds, 
    grid = nn_grid,
    metrics = perf_metrics, 
    control = tune_ctrl
  )
toc()

# Remember to add this if you are running parallel computations.
stopCluster(cl)
```

## Initial Results

```{r}
nn_res %>%
  collect_metrics() %>%
  ggplot(aes(x = hidden_units, 
             y = mean,
             colour = epochs)) +
  geom_point() + 
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err)
  ) + 
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Neural Networks', 
       subtitle = 'Number of Hidden Units and Epochs', 
       x = 'Hidden Units', y = 'Performance Score')
```

## Refinements

We adjust the experiment's parameters and try to optimize further:

```{r}
nn_grid2 <- grid_regular(hidden_units(range = c(7, 20)), 
                        penalty(range = c(-5, 0)),
                        epochs(range = c(200, 500)), 
                        levels = c(5, 5, 2))
```

Tune the model using the new grid.

```{r}
all_cores <- parallel::detectCores(logical = FALSE)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)

tic()
nn_res2 <-
  nn_wf %>%
  tune_grid(
    office_folds, 
    grid = nn_grid2,
    metrics = perf_metrics, 
    control = tune_ctrl
  )
toc()

stopCluster(cl)
```

Produce refined results.

```{r}
nn_res2 %>%
  collect_metrics() %>%
  ggplot(aes(x = hidden_units, 
             y = mean,
             colour = penalty)) +
  geom_point() + 
  geom_errorbar(
    aes(ymin = mean - std_err, 
        ymax = mean + std_err)
  ) + 
  scale_x_log10() + 
  facet_wrap(~.metric, scales = 'free', ncol = 2) +
  theme_minimal() + 
  labs(title = 'Neural Nets', 
       subtitle = 'Number of Hidden Units and Penalty', 
       x = 'Hidden Units', y = 'Performance Score')
```

A different visualization.

```{r}
nn_res2 %>%
  collect_metrics() %>%
  filter(.metric != 'rsq') %>%
  ggplot(aes(x = hidden_units, 
             y = penalty,
             z = mean)) +
  geom_contour_filled() + 
  scale_y_log10() + 
  facet_wrap(~.metric, ncol = 2) +
  theme_minimal() + 
  labs(title = 'Neural Networks', 
       subtitle = 'Hidden Units and Penalty', 
       x = 'Hidden Units', y = 'Penalty')
```

The top performing models by RMSE are:

```{r}
nn_res2 %>%
  show_best(metric = 'rmse') %>%
  kable(digits = 3,
        caption = 'Top-Performing Linear NN Models by RMSE')
```


# Model Selection

We select the best model by RMSE.

```{r}
nn_best <- 
  nn_res2 %>%
  select_best( metric = 'rmse')
nn_best %>%
  kable(digits = 3, 
        caption = 'Top-Performing Model by RMSE')
```


```{r}
nn_by_se <- 
  nn_res2 %>%
  select_by_one_std_err(metric = 'rmse', 'margin')
nn_by_se %>%
  kable(digits = 3, 
        caption = 'Top Performing Model Adjusted by SE (RMSE)')
```

```{r}
nn_res2 %>%
  collect_predictions(parameters = nn_best) %>%
  ggplot(aes(x = imdb_rating, y = .pred)) + 
  geom_point(alpha=.25, 
             color = 'skyblue4') + 
  geom_smooth(method = 'loess', 
              se = FALSE,
              colour = 'orangered3') + 
  geom_abline(slope = 1, 
              intercept = 0, linetype = 2) +
  scale_x_continuous(breaks = breaks_extended(n = 15)) + 
  scale_y_continuous(breaks = breaks_extended(n = 7)) + 
  geom_rug(alpha = 0.25) + 
  theme_minimal() +
  labs(title = 'Predicted and Actual IMBD Ratings', 
       subtitle = 'Loess Smoothing (solid) and 45 Degree Line (dotted)', 
       x = 'IMBD Rating', y = 'Predicted Rating')
```

# Finalize Workflow and Train

```{r}
nn_final <-
  nn_wf %>%
  finalize_workflow(nn_best)

nn_fit <- nn_final %>%
  fit(office_train)

nn_fit
```

# Save Model Objects

```{r}
save(nn_fit, nn_res2, nn_wf, 
     file = './data/nn_res.Rda')
```
